{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf4a537",
   "metadata": {
    "papermill": {
     "duration": 0.02532,
     "end_time": "2022-08-10T21:51:36.976044",
     "exception": false,
     "start_time": "2022-08-10T21:51:36.950724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run a SageMaker Experiment with Pytorch DDP - MNIST Handwritten Digits Classification\n",
    "\n",
    "This notebook shows how you can use the SageMaker SDK to track a Machine Learning experiment. \n",
    "\n",
    "We introduce two concepts in this notebook -\n",
    "\n",
    "* *Experiment:* An experiment is a collection of runs. When you initialize a run in your training loop, you include the name of the experiment that the run belongs to. Experiment names must be unique within your AWS account. \n",
    "* *Run:* A run consists of all the inputs, parameters, configurations, and results for one iteration of model training. Initialize an experiment run for tracking a training job with Run(). \n",
    "\n",
    "To execute this notebook in SageMaker Studio, you should select the `PyTorch 1.12 Python 3.8 CPU Optimizer image`.\n",
    "\n",
    "\n",
    "You can track artifacts for experiments, including datasets, algorithms, hyperparameters and metrics. Experiments executed on SageMaker such as SageMaker training jobs are automatically tracked and any existing SageMaker experiment on your AWS account is automatically migrated to the new UI version.\n",
    "\n",
    "We demonstrate these capabilities through a PyTorch DDP - MNIST handwritten digits classification example. The experiment is organized as follows:\n",
    "\n",
    "1. Download and prepare the MNIST dataset.\n",
    "2. Train a Convolutional Neural Network (CNN) Model. Tune the hyperparameter that configures the number of hidden channels in the model. Track the parameter configurations and resulting model accuracy using the SageMaker Experiments Python SDK.\n",
    "3. Finally use the search and analytics capabilities of the SDK to search, compare and evaluate the performance of all model versions generated from model tuning in Step 2.\n",
    "4. We also show an example of tracing the complete lineage of a model version: the collection of all the data pre-processing and training configurations and inputs that went into creating that model version.\n",
    "\n",
    "Make sure you select the `PyTorch 1.12 Python 3.8 CPU Optimized` kernel in Amazon SageMaker Studio.\n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook takes approximately 25 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Install modules](#Install-modules)\n",
    "1. [Setup](#Setup)\n",
    "1. [Download the dataset](#Download-the-dataset)\n",
    "1. [Step 1: Set up the Experiment](#Step-1:-Set-up-the-Experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27e803",
   "metadata": {
    "papermill": {
     "duration": 0.024762,
     "end_time": "2022-08-10T21:51:37.025914",
     "exception": false,
     "start_time": "2022-08-10T21:51:37.001152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a1cd1f-0c57-4b21-aff7-225fcce94606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d18318-8995-4711-8f76-321fa4654ac9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sagemaker 2.132.0\n",
      "Uninstalling sagemaker-2.132.0:\n",
      "  Successfully uninstalled sagemaker-2.132.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.0.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.26.71)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.79-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.30.0,>=1.29.79\n",
      "  Downloading botocore-1.29.79-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m156.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.79->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.79->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.79->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.71\n",
      "    Uninstalling botocore-1.29.71:\n",
      "      Successfully uninstalled botocore-1.29.71\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.71\n",
      "    Uninstalling boto3-1.26.71:\n",
      "      Successfully uninstalled boto3-1.26.71\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.71 requires botocore==1.29.71, but you have botocore 1.29.79 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.79 botocore-1.29.79\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting sagemaker==2.123.0\n",
      "  Downloading sagemaker-2.123.0.tar.gz (667 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m667.3/667.3 kB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (1.26.79)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (3.20.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (1.4.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker==2.123.0) (0.7.5)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker==2.123.0) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker==2.123.0) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.79 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker==2.123.0) (1.29.79)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker==2.123.0) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->sagemaker==2.123.0) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker==2.123.0) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker==2.123.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker==2.123.0) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker==2.123.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker==2.123.0) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker==2.123.0) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker==2.123.0) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker==2.123.0) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.79->boto3<2.0,>=1.26.28->sagemaker==2.123.0) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.123.0-py2.py3-none-any.whl size=889610 sha256=b82a5d010ff043f51324f9206d4a4ea3090297ae4bcc75e0a034a781764400c1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v9932hsy/wheels/95/8f/9d/be08450efb278b2dbe35ea5f6cc9fc70646ac91593448ffb56\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "Successfully installed sagemaker-2.123.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision) (4.4.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchvision) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "# update boto3 and sagemaker to ensure latest SDK version\n",
    "!{sys.executable} -m pip uninstall -y sagemaker\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade boto3 --no-cache-dir\n",
    "!{sys.executable} -m pip install --upgrade sagemaker==2.123.0 --no-cache-dir\n",
    "!{sys.executable} -m pip install --upgrade torch\n",
    "!{sys.executable} -m pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a6ed2",
   "metadata": {
    "papermill": {
     "duration": 0.025572,
     "end_time": "2022-08-10T21:51:37.132910",
     "exception": false,
     "start_time": "2022-08-10T21:51:37.107338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install the SageMaker Experiments Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c708fd",
   "metadata": {
    "papermill": {
     "duration": 0.808621,
     "end_time": "2022-08-10T21:53:54.461296",
     "exception": false,
     "start_time": "2022-08-10T21:53:53.652675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0baec74a",
   "metadata": {
    "papermill": {
     "duration": 6.893331,
     "end_time": "2022-08-10T21:54:02.167120",
     "exception": false,
     "start_time": "2022-08-10T21:53:55.273789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11680/1041990755.py:18: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats(\"retina\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import importlib\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "set_matplotlib_formats(\"retina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb38697",
   "metadata": {
    "papermill": {
     "duration": 1.907829,
     "end_time": "2022-08-10T21:54:04.863387",
     "exception": false,
     "start_time": "2022-08-10T21:54:02.955558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "sess = sm_sess.boto_session\n",
    "sm = sm_sess.sagemaker_client\n",
    "role = get_execution_role()\n",
    "region = sess.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f67a29",
   "metadata": {
    "papermill": {
     "duration": 0.796698,
     "end_time": "2022-08-10T21:54:06.154129",
     "exception": false,
     "start_time": "2022-08-10T21:54:05.357431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download the dataset\n",
    "We download the MNIST handwritten digits dataset, and then apply a transformation on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5afddda",
   "metadata": {
    "papermill": {
     "duration": 4.705622,
     "end_time": "2022-08-10T21:54:11.677415",
     "exception": false,
     "start_time": "2022-08-10T21:54:06.971793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using S3 location: s3://sagemaker-us-west-1-030086719966/DEMO-mnist/\n"
     ]
    }
   ],
   "source": [
    "bucket = sm_sess.default_bucket()\n",
    "prefix = \"DEMO-mnist\"\n",
    "print(\"Using S3 location: s3://\" + bucket + \"/\" + prefix + \"/\")\n",
    "\n",
    "datasets.MNIST.urls = [\n",
    "    \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\",\n",
    "    \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\",\n",
    "    \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\",\n",
    "    \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\",\n",
    "]\n",
    "\n",
    "# Download the dataset to the ./mnist folder, and load and transform (normalize) them\n",
    "train_set = datasets.MNIST(\n",
    "    \"mnist\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    \"mnist\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    "    download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d2cf2",
   "metadata": {
    "papermill": {
     "duration": 0.424392,
     "end_time": "2022-08-10T21:54:12.574066",
     "exception": false,
     "start_time": "2022-08-10T21:54:12.149674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "View an example image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa9b226",
   "metadata": {
    "papermill": {
     "duration": 1.398533,
     "end_time": "2022-08-10T21:54:14.255018",
     "exception": false,
     "start_time": "2022-08-10T21:54:12.856485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8e1639b2e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 416
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_set.data[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095111d",
   "metadata": {
    "papermill": {
     "duration": 0.812931,
     "end_time": "2022-08-10T21:54:15.860198",
     "exception": false,
     "start_time": "2022-08-10T21:54:15.047267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After transforming the images in the dataset, we upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5381859",
   "metadata": {
    "papermill": {
     "duration": 5.752617,
     "end_time": "2022-08-10T21:54:22.428962",
     "exception": false,
     "start_time": "2022-08-10T21:54:16.676345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 path for data:  s3://sagemaker-us-west-1-030086719966/DEMO-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker.Session().upload_data(path=\"mnist\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"S3 path for data: \", inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349b52b-c984-4924-978f-a53b6e646071",
   "metadata": {},
   "source": [
    "## Prepare Training Script for Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5064acca-c600-4797-b59b-996d3abd2329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./mnist_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./mnist_ddp.py\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "os.system(\"pip install -U sagemaker\")\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.experiments.run import Run, load_run\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "boto_session = boto3.session.Session(region_name=os.environ[\"AWS_REGION\"])\n",
    "sagemaker_session = Session(boto_session=boto_session)\n",
    "\n",
    "\n",
    "if \"SAGEMAKER_METRICS_DIRECTORY\" in os.environ:\n",
    "    log_file_handler = logging.FileHandler(\n",
    "        join(os.environ[\"SAGEMAKER_METRICS_DIRECTORY\"], \"metrics.json\")\n",
    "    )\n",
    "    formatter = logging.Formatter(\n",
    "        \"{'time':'%(asctime)s', 'name': '%(name)s', \\\n",
    "    'level': '%(levelname)s', 'message': '%(message)s'}\",\n",
    "        style=\"%\",\n",
    "    )\n",
    "    log_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(log_file_handler)\n",
    "\n",
    "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_channels, kernel_size, drop_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, hidden_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, 20, kernel_size=kernel_size)\n",
    "        self.conv2_drop = nn.Dropout2d(p=drop_out)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def _get_train_data_loader(batch_size, training_dir, is_distributed, **kwargs):\n",
    "    logger.info(\"Get train data loader\")\n",
    "    dataset = datasets.MNIST(\n",
    "        training_dir,\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=False,\n",
    "    )\n",
    "    train_sampler = (\n",
    "        torch.utils.data.distributed.DistributedSampler(dataset) if is_distributed else None\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train_sampler is None,\n",
    "        sampler=train_sampler,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_test_data_loader(test_batch_size, training_dir, **kwargs):\n",
    "    logger.info(\"Get test data loader\")\n",
    "    return torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            training_dir,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "            download=False,\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def _average_gradients(model):\n",
    "    # Gradient averaging.\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
    "        param.grad.data /= size\n",
    "\n",
    "\n",
    "def train(args, tracker=None):\n",
    "    print(\"------ number of hosts --------\", len(args.hosts))\n",
    "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "    logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    rank = None\n",
    "\n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(args.hosts)\n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        host_rank = args.hosts.index(args.current_host)\n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
    "        rank = dist.get_rank()\n",
    "        print(\"------- rank --------\", rank)\n",
    "        logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                args.backend, dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), args.num_gpus)\n",
    "        )\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
    "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\n",
    "\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = Net(args.hidden_channels, args.kernel_size, args.dropout).to(device)\n",
    "    if is_distributed and use_cuda:\n",
    "        # multi-machine multi-gpu case\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    else:\n",
    "        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    if args.optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    with load_run(sagemaker_session=sagemaker_session) as run:\n",
    "        run.log_parameters(vars(args))\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            model.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                if is_distributed and not use_cuda:\n",
    "                    # average gradients manually for multi-machine cpu case only\n",
    "                    _average_gradients(model)\n",
    "                optimizer.step()\n",
    "                if batch_idx % args.log_interval == 0 and rank == 0:\n",
    "                    logger.info(\n",
    "                        \"Train Epoch: {} [{}/{} ({:.0f}%)], Train Loss: {:.6f};\".format(\n",
    "                            epoch,\n",
    "                            batch_idx * len(data),\n",
    "                            len(train_loader.sampler),\n",
    "                            100.0 * batch_idx / len(train_loader),\n",
    "                            loss.item(),\n",
    "                        )\n",
    "                    )\n",
    "            if rank == 0:\n",
    "                test_loss, correct, target, pred = test(model, test_loader, device, tracker)\n",
    "                logger.info(\n",
    "                    \"Test Average loss: {:.4f}, Test Accuracy: {:.0f}%;\\n\".format(\n",
    "                        test_loss, 100.0 * correct / len(test_loader.dataset)\n",
    "                    )\n",
    "                )\n",
    "                run.log_metric(name=\"train_loss\", value=loss.item(), step=epoch)\n",
    "                run.log_metric(name=\"test_loss\", value=test_loss, step=epoch)\n",
    "                run.log_metric(\n",
    "                    name=\"test_accuracy\",\n",
    "                    value=100.0 * correct / len(test_loader.dataset),\n",
    "                    step=epoch,\n",
    "                )\n",
    "    save_model(model, args.model_dir)\n",
    "\n",
    "\n",
    "def test(model, test_loader, device, tracker=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss, correct, target, pred\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    hidden_channels = int(os.environ.get(\"hidden_channels\", \"5\"))\n",
    "    kernel_size = int(os.environ.get(\"kernel_size\", \"5\"))\n",
    "    dropout = float(os.environ.get(\"dropout\", \"0.5\"))\n",
    "    model = torch.nn.DataParallel(Net(hidden_channels, kernel_size, dropout))\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "        return model.to(device)\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"sgd\", help=\"optimizer for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        metavar=\"LR\",\n",
    "        help=\"learning rate (default: 0.01)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        metavar=\"DROP\",\n",
    "        help=\"dropout rate (default: 0.5)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kernel_size\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        metavar=\"KERNEL\",\n",
    "        help=\"conv2d filter kernel size (default: 5)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--momentum\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        metavar=\"M\",\n",
    "        help=\"SGD momentum (default: 0.5)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_channels\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of channels in hidden conv layer\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backend\",\n",
    "        type=str,\n",
    "        default=\"nccl\",\n",
    "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef9e20",
   "metadata": {
    "papermill": {
     "duration": 0.806991,
     "end_time": "2022-08-10T21:54:27.281808",
     "exception": false,
     "start_time": "2022-08-10T21:54:26.474817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Set up the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc161575",
   "metadata": {
    "papermill": {
     "duration": 0.72498,
     "end_time": "2022-08-10T21:54:28.880347",
     "exception": false,
     "start_time": "2022-08-10T21:54:28.155367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e392c92e",
   "metadata": {
    "papermill": {
     "duration": 0.912762,
     "end_time": "2022-08-10T21:54:34.077683",
     "exception": false,
     "start_time": "2022-08-10T21:54:33.164921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d272ee",
   "metadata": {
    "papermill": {
     "duration": 0.804412,
     "end_time": "2022-08-10T21:54:37.481946",
     "exception": false,
     "start_time": "2022-08-10T21:54:36.677534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you want to run the following five training jobs in parallel, you may need to increase your resource limit. Here we run them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef52c1e7-5881-4e41-a9e5-b7ceb6900d40",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (run-ddp-1) under experiment (distributed-train-job-experiment-final) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-02-28-09-01-54-152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-28 09:01:54 Starting - Starting the training job...\n",
      "2023-02-28 09:02:20 Starting - Preparing the instances for training......\n",
      "2023-02-28 09:03:19 Downloading - Downloading input data...\n",
      "2023-02-28 09:03:39 Training - Downloading the training image...\n",
      "2023-02-28 09:04:04 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,446 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,447 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,449 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,460 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,462 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,679 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,681 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,694 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,696 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,711 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,713 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:29,724 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c4.8xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"dropout\": 0.2,\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 20,\n",
      "        \"kernel_size\": 5,\n",
      "        \"optimizer\": \"sgd\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c4.8xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-02-28-09-01-54-152\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-1-030086719966/pytorch-training-2023-02-28-09-01-54-152/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ddp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 36,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c4.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c4.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ddp.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"dropout\":0.2,\"epochs\":10,\"hidden_channels\":20,\"kernel_size\":5,\"optimizer\":\"sgd\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_ddp.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c4.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c4.8xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_ddp\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=36\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-1-030086719966/pytorch-training-2023-02-28-09-01-54-152/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.c4.8xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"dropout\":0.2,\"epochs\":10,\"hidden_channels\":20,\"kernel_size\":5,\"optimizer\":\"sgd\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-02-28-09-01-54-152\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-1-030086719966/pytorch-training-2023-02-28-09-01-54-152/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ddp\",\"network_interface_name\":\"eth0\",\"num_cpus\":36,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c4.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ddp.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--dropout\",\"0.2\",\"--epochs\",\"10\",\"--hidden_channels\",\"20\",\"--kernel_size\",\"5\",\"--optimizer\",\"sgd\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=20\u001b[0m\n",
      "\u001b[34mSM_HP_KERNEL_SIZE=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=sgd\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist_ddp.py --backend gloo --dropout 0.2 --epochs 10 --hidden_channels 20 --kernel_size 5 --optimizer sgd\u001b[0m\n",
      "\u001b[34m2023-02-28 09:04:30,367 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.132.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.135.0.tar.gz (673 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 673.8/673.8 kB 20.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.70)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.70 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.70)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.70->boto3<2.0,>=1.26.28->sagemaker) (1.26.14)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:28,858 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:28,859 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:28,861 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:28,872 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:28,874 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,103 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,105 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,118 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,120 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,134 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,136 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,147 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c4.8xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"dropout\": 0.2,\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 20,\n",
      "        \"kernel_size\": 5,\n",
      "        \"optimizer\": \"sgd\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c4.8xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-02-28-09-01-54-152\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-1-030086719966/pytorch-training-2023-02-28-09-01-54-152/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_ddp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 36,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.c4.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c4.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_ddp.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"dropout\":0.2,\"epochs\":10,\"hidden_channels\":20,\"kernel_size\":5,\"optimizer\":\"sgd\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist_ddp.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.c4.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.c4.8xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist_ddp\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=36\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-1-030086719966/pytorch-training-2023-02-28-09-01-54-152/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.c4.8xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"dropout\":0.2,\"epochs\":10,\"hidden_channels\":20,\"kernel_size\":5,\"optimizer\":\"sgd\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-02-28-09-01-54-152\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-1-030086719966/pytorch-training-2023-02-28-09-01-54-152/source/sourcedir.tar.gz\",\"module_name\":\"mnist_ddp\",\"network_interface_name\":\"eth0\",\"num_cpus\":36,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.c4.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c4.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_ddp.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--dropout\",\"0.2\",\"--epochs\",\"10\",\"--hidden_channels\",\"20\",\"--kernel_size\",\"5\",\"--optimizer\",\"sgd\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_DROPOUT=0.2\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[35mSM_HP_HIDDEN_CHANNELS=20\u001b[0m\n",
      "\u001b[35mSM_HP_KERNEL_SIZE=5\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZER=sgd\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 mnist_ddp.py --backend gloo --dropout 0.2 --epochs 10 --hidden_channels 20 --kernel_size 5 --optimizer sgd\u001b[0m\n",
      "\u001b[35m2023-02-28 09:04:29,789 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.132.0)\u001b[0m\n",
      "\u001b[35mCollecting sagemaker\u001b[0m\n",
      "\u001b[35mDownloading sagemaker-2.135.0.tar.gz (673 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 673.8/673.8 kB 23.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.70)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.20.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (23.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: botocore<1.30.0,>=1.29.70 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.70)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.70->boto3<2.0,>=1.26.28->sagemaker) (1.26.14)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[35mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for sagemaker: filename=sagemaker-2.135.0-py2.py3-none-any.whl size=911428 sha256=8335537e28f5a9ca1de56a06197f370408d04b0a17049beac55cff6953a7490a\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/ce/c2/3f/c92ead9fc44bb07546f1cba2aa9cb93d01b8f8e52a787e62ec\u001b[0m\n",
      "\u001b[35mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[35mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[35mFound existing installation: sagemaker 2.132.0\u001b[0m\n",
      "\u001b[35mUninstalling sagemaker-2.132.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled sagemaker-2.132.0\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.135.0-py2.py3-none-any.whl size=911428 sha256=c0f766cad12237cf439875fca35060e2d5c6a90cf9f2a02327cf8b2c42b9132a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ce/c2/3f/c92ead9fc44bb07546f1cba2aa9cb93d01b8f8e52a787e62ec\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[35mSuccessfully installed sagemaker-2.135.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.132.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.132.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.132.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.135.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m------ number of hosts -------- 2\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Distributed training - True\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Number of gpus available - 0\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34m------ number of hosts -------- 2\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Distributed training - True\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Number of gpus available - 0\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[35mINFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\u001b[0m\n",
      "\u001b[35m------- rank -------- 1\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mGet train data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[35mGet test data loader\u001b[0m\n",
      "\u001b[35mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[35mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Processes 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mINFO:sagemaker.experiments.run:The run (run-ddp-1) under experiment (distributed-train-job-experiment-final) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[35m[2023-02-28 09:04:37.065 algo-2:75 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\u001b[0m\n",
      "\u001b[34m------- rank -------- 0\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Get test data loader\u001b[0m\n",
      "\u001b[34mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (run-ddp-1) under experiment (distributed-train-job-experiment-final) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34m[2023-02-28 09:04:37.024 algo-1:75 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-02-28 09:04:37.203 algo-1:75 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-02-28 09:04:37.205 algo-1:75 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-02-28 09:04:37.205 algo-1:75 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-02-28 09:04:37.206 algo-1:75 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-02-28 09:04:37.206 algo-1:75 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[35m[2023-02-28 09:04:37.245 algo-2:75 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-02-28 09:04:37.246 algo-2:75 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-02-28 09:04:37.246 algo-2:75 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-02-28 09:04:37.247 algo-2:75 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-02-28 09:04:37.247 algo-2:75 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/30000 (21%)], Train Loss: 2.003852;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/30000 (21%)], Train Loss: 2.003852;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/30000 (43%)], Train Loss: 0.956334;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/30000 (43%)], Train Loss: 0.956334;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/30000 (64%)], Train Loss: 0.749969;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/30000 (64%)], Train Loss: 0.749969;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/30000 (85%)], Train Loss: 0.516511;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/30000 (85%)], Train Loss: 0.516511;\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.2314, Test Accuracy: 93%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.2314, Test Accuracy: 93%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/30000 (21%)], Train Loss: 0.434608;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/30000 (21%)], Train Loss: 0.434608;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/30000 (43%)], Train Loss: 0.441903;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/30000 (43%)], Train Loss: 0.441903;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/30000 (64%)], Train Loss: 0.619932;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/30000 (64%)], Train Loss: 0.619932;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/30000 (85%)], Train Loss: 0.204416;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/30000 (85%)], Train Loss: 0.204416;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1345, Test Accuracy: 96%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1345, Test Accuracy: 96%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/30000 (21%)], Train Loss: 0.278287;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/30000 (21%)], Train Loss: 0.278287;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/30000 (43%)], Train Loss: 0.169433;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/30000 (43%)], Train Loss: 0.169433;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/30000 (64%)], Train Loss: 0.432077;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/30000 (64%)], Train Loss: 0.432077;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/30000 (85%)], Train Loss: 0.196588;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/30000 (85%)], Train Loss: 0.196588;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1068, Test Accuracy: 97%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1068, Test Accuracy: 97%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/30000 (21%)], Train Loss: 0.218571;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/30000 (21%)], Train Loss: 0.218571;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/30000 (43%)], Train Loss: 0.248072;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/30000 (43%)], Train Loss: 0.248072;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/30000 (64%)], Train Loss: 0.315881;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/30000 (64%)], Train Loss: 0.315881;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/30000 (85%)], Train Loss: 0.281154;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/30000 (85%)], Train Loss: 0.281154;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0906, Test Accuracy: 97%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0906, Test Accuracy: 97%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/30000 (21%)], Train Loss: 0.146505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/30000 (21%)], Train Loss: 0.146505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/30000 (43%)], Train Loss: 0.209262;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/30000 (43%)], Train Loss: 0.209262;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/30000 (64%)], Train Loss: 0.502211;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/30000 (64%)], Train Loss: 0.502211;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/30000 (85%)], Train Loss: 0.141908;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/30000 (85%)], Train Loss: 0.141908;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0778, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0778, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/30000 (21%)], Train Loss: 0.223637;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/30000 (21%)], Train Loss: 0.223637;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/30000 (43%)], Train Loss: 0.154856;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/30000 (43%)], Train Loss: 0.154856;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/30000 (64%)], Train Loss: 0.271969;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/30000 (64%)], Train Loss: 0.271969;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/30000 (85%)], Train Loss: 0.089652;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/30000 (85%)], Train Loss: 0.089652;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0702, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0702, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/30000 (21%)], Train Loss: 0.169512;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/30000 (21%)], Train Loss: 0.169512;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/30000 (43%)], Train Loss: 0.133054;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/30000 (43%)], Train Loss: 0.133054;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/30000 (64%)], Train Loss: 0.279522;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/30000 (64%)], Train Loss: 0.279522;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/30000 (85%)], Train Loss: 0.089789;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/30000 (85%)], Train Loss: 0.089789;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0640, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0640, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/30000 (21%)], Train Loss: 0.234327;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/30000 (21%)], Train Loss: 0.234327;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/30000 (43%)], Train Loss: 0.125899;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/30000 (43%)], Train Loss: 0.125899;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/30000 (64%)], Train Loss: 0.278218;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/30000 (64%)], Train Loss: 0.278218;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/30000 (85%)], Train Loss: 0.049247;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/30000 (85%)], Train Loss: 0.049247;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0607, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0607, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/30000 (21%)], Train Loss: 0.123588;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/30000 (21%)], Train Loss: 0.123588;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/30000 (43%)], Train Loss: 0.108004;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/30000 (43%)], Train Loss: 0.108004;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/30000 (64%)], Train Loss: 0.264463;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/30000 (64%)], Train Loss: 0.264463;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/30000 (85%)], Train Loss: 0.120503;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/30000 (85%)], Train Loss: 0.120503;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0557, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0557, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/30000 (21%)], Train Loss: 0.190715;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/30000 (21%)], Train Loss: 0.190715;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/30000 (43%)], Train Loss: 0.158019;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/30000 (43%)], Train Loss: 0.158019;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/30000 (64%)], Train Loss: 0.275456;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/30000 (64%)], Train Loss: 0.275456;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/30000 (85%)], Train Loss: 0.141428;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/30000 (85%)], Train Loss: 0.141428;\u001b[0m\n",
      "\u001b[35mSaving the model.\u001b[0m\n",
      "\u001b[35mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[35m2023-02-28 09:06:37,161 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-02-28 09:06:37,161 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-02-28 09:06:37,161 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.0523, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.0523, Test Accuracy: 98%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-02-28 09:06:39,697 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-02-28 09:06:39,697 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-02-28 09:06:39,697 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-02-28 09:06:56 Uploading - Uploading generated training model\n",
      "2023-02-28 09:06:56 Completed - Training job completed\n",
      "Training seconds: 436\n",
      "Billable seconds: 436\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.experiments.run import Run\n",
    "\n",
    "experiment_name = \"distributed-train-job-experiment-final\"\n",
    "run_name = \"run-ddp-1\"\n",
    "with Run(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=run_name,\n",
    "    sagemaker_session=sm_sess,\n",
    ") as run:\n",
    "    est = PyTorch(\n",
    "        entry_point=\"./mnist_ddp.py\",\n",
    "        role=role,\n",
    "        model_dir=False,\n",
    "        framework_version=\"1.12\",\n",
    "        py_version=\"py38\",\n",
    "        instance_type=\"ml.c4.8xlarge\",\n",
    "        instance_count=2,\n",
    "        hyperparameters={\n",
    "            \"epochs\": 10,\n",
    "            \"hidden_channels\": 20,\n",
    "            \"backend\": \"gloo\",\n",
    "            \"dropout\": 0.2,\n",
    "            \"kernel_size\": 5,\n",
    "            \"optimizer\": \"sgd\",\n",
    "        },\n",
    "        keep_alive_period_in_seconds=10 * 60,  # keeping the instance warm for 10mins\n",
    "    )\n",
    "    est.fit(\n",
    "        inputs={\"training\": inputs},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1889.723815,
   "end_time": "2022-08-10T22:23:05.721503",
   "environment_variables": {},
   "exception": null,
   "input_path": "mnist-handwritten-digits-classification-experiment.ipynb",
   "output_path": "/opt/ml/processing/output/mnist-handwritten-digits-classification-experiment-2022-08-10-21-39-25.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:000000000000:1234abcd-12ab-34cd-56ef-1234567890ab"
   },
   "start_time": "2022-08-10T21:51:35.997688",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
